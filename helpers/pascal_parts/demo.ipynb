{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get annotations in a format that can be written in the darknet fromat. So .txt files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shutil import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from skimage.io import imread\n",
    "from VOClabelcolormap import color_map\n",
    "from anno import ImageAnnotation\n",
    "import glob\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anno_paths = glob.glob(\"Annotations_Part/*.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Demo for showing the parts on an image\n",
    "for anno in anno_paths[:100]:\n",
    "    im_path = \"../VOC2010/VOCdevkit/VOC2010/JPEGImages/\" + anno.rstrip(\".mat\").split(\"/\")[1] + \".jpg\"\n",
    "    \n",
    "    an = ImageAnnotation(im_path, anno)\n",
    "    \n",
    "    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "    ax1.imshow(an.im)\n",
    "    ax1.set_title('Image')\n",
    "    ax1.axis('off')\n",
    "    ax2.imshow(an.cls_mask, cmap=color_map(N=np.max(an.cls_mask) + 1))\n",
    "    ax2.set_title('Class mask')\n",
    "    ax2.axis('off')\n",
    "    ax3.imshow(an.inst_mask, cmap=color_map(N=np.max(an.inst_mask) + 1))\n",
    "    ax3.set_title('Instance mask')\n",
    "    ax3.axis('off')\n",
    "    if np.max(an.part_mask) == 0:\n",
    "        ax4.imshow(an.part_mask, cmap='gray')\n",
    "    else:\n",
    "        ax4.imshow(an.part_mask, cmap=color_map(N=np.max(an.part_mask) + 1))\n",
    "    ax4.set_title('Part mask')\n",
    "    ax4.axis('off')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_enclosing_bb(combined_mask, show = True):\n",
    "    # The function receives as input a combined mask where the mask pixels are asserted where there is a part\n",
    "    # The function return the smallest possible upright bounding box which encloses all the white pixel  \n",
    "        # A cool idea would be to return a non-upright bounding box which encloses all the white pixels but this would entail a shift in training strategy as yolo would now require for us to give an extra parameter of rotation. This may not be the best idea but worth exploring\n",
    "    \n",
    "    # To get a minimum enclosing rectangle we need to get the minimum \n",
    "    \n",
    "    contours, hierarchy = cv2.findContours(combined_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    height, width = combined_mask.shape\n",
    "    min_x, min_y = width, height\n",
    "    max_x = max_y = 0\n",
    "\n",
    "    # computes the bounding box for the contour, and draws it on the frame,\n",
    "    for contour in contours:\n",
    "        (x,y,w,h) = cv2.boundingRect(contour)\n",
    "        min_x, max_x = min(x, min_x), max(x+w, max_x)\n",
    "        min_y, max_y = min(y, min_y), max(y+h, max_y)\n",
    "        \n",
    "    w = max_x - min_x\n",
    "    h = max_y - min_y\n",
    "    xc = min_x + (w/2)\n",
    "    yc = min_y + (h/2)\n",
    "    \n",
    "#     # Debugging\n",
    "#     if show:\n",
    "# #         plt.imshow(combined_mask, cmap=color_map(N=np.max(combined_mask) + 1))\n",
    "#         cv2.rectangle(combined_mask, (min_x, min_y), (max_x, max_y), (255, 0, 0), 2)\n",
    "#         f, (ax1) = plt.subplots(1, 1)\n",
    "# #         , cmap=color_map(N=np.max(combined_mask) + 1)\n",
    "#         ax1.imshow(combined_mask)\n",
    "#         ax1.set_title('Image')\n",
    "#         ax1.axis('off')\n",
    "#         plt.show()\n",
    "    \n",
    "    \n",
    "    # normalise the coordinates\n",
    "    w = w/width\n",
    "    h = h/height\n",
    "    xc = xc/width\n",
    "    yc = yc/height\n",
    "    \n",
    "    return xc, yc, w, h, (min_x, min_y), (max_x, max_y)\n",
    "\n",
    "def get_combined_mask(an_obj, sub_part):\n",
    "    # The sub parts array is a tuple which contains body parts.\n",
    "    # The function returns a combined mask for the available body parts if they exist, otherwise that part just doesnt exist in the current image\n",
    "    # for example a head doesnt exist for the current an_object then we just skip that body part\n",
    "    \n",
    "    # Send an_object.mask if 'all' in subparts\n",
    "    if 'all' in sub_part:\n",
    "        return an_obj.mask\n",
    "\n",
    "    # First we make a buffer mask which will contain all the amalgamated mask\n",
    "    buffer_mask = np.zeros((an_obj.mask.shape), dtype = np.uint8)\n",
    "    \n",
    "    if 'frontal_face' in sub_part:\n",
    "#         # Here we will return the head mask but with the hair mask subtracted\n",
    "#         hair_mask = neck_mask = np.zeros_like(an_obj.mask, dtype = np.uint8)\n",
    "        \n",
    "#         for tiny_part in an_obj.parts:\n",
    "#             if tiny_part.part_name == 'head':\n",
    "#                 buffer_mask += tiny_part.mask\n",
    "#             if tiny_part.part_name == 'hair':\n",
    "#                 hair_mask = tiny_part.mask\n",
    "#             if tiny_part.part_name == 'neck':\n",
    "#                 neck_mask = tiny_part.mask\n",
    "            \n",
    "                \n",
    "#         # Here we extend the hair mask slightly beyond its boundaries to get a cleaner cut\n",
    "#         kernel = np.ones((100,100),np.uint8)\n",
    "#         cv2.dilate(hair_mask, kernel, iterations = 1)\n",
    "#         cv2.dilate(neck_mask, kernel, iterations = 1)\n",
    "        \n",
    "#         buffer_mask -= hair_mask\n",
    "#         buffer_mask[buffer_mask == 255] = 0\n",
    "        \n",
    "#         buffer_mask -= neck_mask\n",
    "#         buffer_mask[buffer_mask == 255] = 0\n",
    "        \n",
    "#         return buffer_mask\n",
    "#         frontal_face = set(['lear', 'rear', 'mouth', 'leye', 'reye', 'lebrow', 'rebrow'])\n",
    "        frontal_face = set(['mouth', 'leye', 'reye', 'lebrow', 'rebrow'])\n",
    "    \n",
    "        # For a completely frontal face we need to be able to locate all of the above\n",
    "        # For a partial frontal face we need to be able to locate atleast one of the ears. This could be the logic\n",
    "        all_parts = [x.part_name for x in an_obj.parts]\n",
    "        all_parts = set(all_parts)\n",
    "        \n",
    "        if all_parts == all_parts.union(frontal_face):\n",
    "            # then this is a frontal face. We can just return the head here\n",
    "            for tiny_part in an_obj.parts:\n",
    "                if tiny_part.part_name == 'head':                    \n",
    "                    return tiny_part.mask\n",
    "#             hair_mask = neck_mask = np.zeros_like(an_obj.mask, dtype = np.uint8)\n",
    "#             for tiny_part in an_obj.parts:\n",
    "#                 if tiny_part.part_name == 'head':\n",
    "#                     buffer_mask += tiny_part.mask\n",
    "#                 if tiny_part.part_name == 'hair':\n",
    "#                     hair_mask = tiny_part.mask\n",
    "#                 if tiny_part.part_name == 'neck':\n",
    "#                     neck_mask = tiny_part.mask\n",
    " \n",
    "#             buffer_mask -= hair_mask\n",
    "#             buffer_mask[buffer_mask == 255] = 0\n",
    "\n",
    "#             buffer_mask -= neck_mask\n",
    "#             buffer_mask[buffer_mask == 255] = 0\n",
    "            \n",
    "#             # Now find the biggest contour and return a mask that satisfies it\n",
    "#             contours, hierarchy = cv2.findContours(buffer_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)   \n",
    "            \n",
    "#             # Find the biggest area\n",
    "#             c = max(contours, key = cv2.contourArea)\n",
    "#             x,y,w,h = cv2.boundingRect(c)\n",
    "#             height, width = buffer_mask.shape\n",
    "#             xc = (x+(w/2))/width\n",
    "#             yc = (y+(h/2))/width\n",
    "#             w = w/width\n",
    "#             h = h/height\n",
    "            \n",
    "#             return xc,yc,w,h\n",
    "        else:\n",
    "            # Return an empty mask if the head doest not contain all the required parts\n",
    "            return buffer_mask\n",
    "        \n",
    "\n",
    "    \n",
    "    for tiny_part in an_obj.parts:    \n",
    "        # Now we check whether the tiny part is in our sub_part list\n",
    "        if tiny_part.part_name in sub_part:\n",
    "            buffer_mask += tiny_part.mask\n",
    "        else:\n",
    "            continue \n",
    "    \n",
    "    # It should be noted that the buffer mask could be empty in case where the parts are not available.\n",
    "    return buffer_mask\n",
    "        \n",
    "\n",
    "def has_person(an, PERSON_IND = 15):\n",
    "    if PERSON_IND in np.unique(an.cls_mask):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_bbs(an_obj, parts, parts_stat, image = None):\n",
    "    # This function is responsible for handling the annotations for a single person\n",
    "    # It will also generate bounding boxes for each (combined) part\n",
    "    # For this object it return for example 3 bounding boxes [(0, xc,yc,w,h), (1, xc, yc, w, h), (2, xc,yc,w,h)] for the three parts [all, head, (torso)]\n",
    "    \n",
    "    annot_per_part = []\n",
    "    for index, sub_part in enumerate(parts):\n",
    "        # Get a combined mask for the sub_part for example a mask that encampasses [ear, hair] or [head, left_shoulder, right_shoulder]\n",
    "        combined_mask = get_combined_mask(an_obj, sub_part)\n",
    "\n",
    "        if type(combined_mask) == \"tuple\":\n",
    "            parts_stat[index] += 1\n",
    "            xc,yc,w,h = combined_mask\n",
    "            annot_per_part.append([index, xc,yc,w,h])\n",
    "            continue\n",
    "        \n",
    "        # Check if this combined mask is empty:\n",
    "        if len(np.unique(combined_mask)) > 1:\n",
    "            \n",
    "            parts_stat[index] += 1\n",
    "            \n",
    "            xc,yc,w,h, p1, p2 = get_enclosing_bb(combined_mask)\n",
    "            \n",
    "#             if type(image) != \"NoneType\":\n",
    "#                 cv2.rectangle(image, p1, p2, (255, 0, 0), 2)\n",
    "#                 f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 20))\n",
    "#                 ax1.imshow(image)\n",
    "#                 ax1.set_title('Image')\n",
    "#                 ax1.axis('off')\n",
    "#                 print(combined_mask.dtype)\n",
    "#                 cv2.rectangle(combined_mask, p1, p2, (0, 255, 200), 2)\n",
    "#                 ax2.imshow(combined_mask)\n",
    "#                 ax2.set_title('Mask BB')\n",
    "#                 ax2.axis('off')             \n",
    "#                 plt.show()\n",
    "            \n",
    "            # Make sure these are normalised\n",
    "            annot = [index, xc, yc, w, h]\n",
    "\n",
    "            # And now we append the annotation for this sub part to the list that we will return to our main func\n",
    "            annot_per_part.append(annot)\n",
    "        \n",
    "    return annot_per_part, parts_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parts2darknet(parts, anno_paths, darknet_path = \"darknet/darknet_annots/aug_pf/\"):\n",
    "    # Parts is a list of tuples\n",
    "    # for example: [('head'), ('torso', 'left_lower_leg', 'right_lower_leg'), ('all')]\n",
    "\n",
    "    # Traverse each annotation\n",
    "    # Each Annotation is its own image so for now lets store\n",
    "\n",
    "    # The function will return a dictionary where each valid image's name will become the key and have a list of darknet annotations\n",
    "    # for values. So for examples\n",
    "\n",
    "    parts_stat = [0] * len(parts)\n",
    "    \n",
    "    n_person = 0\n",
    "    n_neg_frames = 0\n",
    "    n_objects = 0\n",
    "    \n",
    "    anno_dict = dict()\n",
    "    for path in tqdm.tqdm(anno_paths):\n",
    "        im_id = path.rstrip(\".mat\").split(\"/\")[1]\n",
    "        im_path = \"../VOC2010/VOCdevkit/VOC2010/JPEGImages/\" + im_id + \".jpg\"\n",
    "        an = ImageAnnotation(im_path, path)\n",
    "\n",
    "#         print(f\"Processing {im_id}\")\n",
    "        \n",
    "        if has_person(an) == False:\n",
    "            # Breaks early out of loop\n",
    "            n_neg_frames += 1\n",
    "            \n",
    "            # Add an empty annot to the dict\n",
    "            anno_dict[im_id] = []\n",
    "            \n",
    "            continue\n",
    "\n",
    "        ##################\n",
    "        #### Augment Image\n",
    "        ##################\n",
    "        # Add new augmented data to the anno_dict\n",
    "        # We will need to read the anno_dict for annotations\n",
    "        # Because we know that our camera will be static we can be sure that only our subjects will be blurred out due to motion\n",
    "        # SO we will utilise this knowledge to motion blur only the subjects which are potentially candidates for motion blur. Which are all persons\n",
    "        # When ever we find a anno dict key which has \n",
    "        # Here we append augmented data to the dictionary\n",
    "            # The problem is that afterwards we copy the images\n",
    "\n",
    "        # Read the Image\n",
    "        aug_img = np.copy(an.im)\n",
    "\n",
    "        # Apply Motion Blur in several Directions\n",
    "        v, h, d, o = apply_motion_blur(aug_img, k_size=10)\n",
    "\n",
    "        # Adjust the bounding boxes \n",
    "            # No need to adjust the bounding box\n",
    "\n",
    "        # Add person_bbs to the anno_dict against a new aug_0_{im_id} <- key\n",
    "        v_id = f\"aug_v_{im_id}\"\n",
    "        h_id = f\"aug_h_{im_id}\"\n",
    "        d_id = f\"aug_d_{im_id}\"\n",
    "        o_id = f\"aug_o_{im_id}\"\n",
    "        \n",
    "        # Now copy augmented images to the the folder\n",
    "        cv2.imwrite(darknet_path + v_id+\".jpg\", v)\n",
    "        cv2.imwrite(darknet_path + h_id+\".jpg\", h)\n",
    "        cv2.imwrite(darknet_path + d_id+\".jpg\", d)\n",
    "        cv2.imwrite(darknet_path + o_id+\".jpg\", o)\n",
    "            \n",
    "        # Traverse the objects\n",
    "        for obj in an.objects:\n",
    "            n_objects += 1\n",
    "            \n",
    "            # Check if person exists in this an\n",
    "            if obj.class_name == 'person':\n",
    "                n_person += 1\n",
    "                \n",
    "                # The chunk of the work happens here: Bounding boxes and classes are generated here\n",
    "#                 person_bbs, parts_count = generate_bbs(obj, parts, parts_stat, image= an.im)\n",
    "                person_bbs, parts_count = generate_bbs(obj, parts, parts_stat)\n",
    "                \n",
    "                # Here we append these bbs to our dictionary\n",
    "                if im_id not in anno_dict.keys():\n",
    "                    anno_dict[im_id] = []\n",
    "                    anno_dict[v_id] = []\n",
    "                    anno_dict[h_id] = []\n",
    "                    anno_dict[d_id] = []\n",
    "                    anno_dict[o_id] = []\n",
    "\n",
    "                # Appending new annotations to the dictionary\n",
    "                for bb in person_bbs:\n",
    "                    anno_dict[im_id].append(bb)\n",
    "                    anno_dict[v_id].append(bb)\n",
    "                    anno_dict[h_id].append(bb)\n",
    "                    anno_dict[d_id].append(bb)\n",
    "                    anno_dict[o_id].append(bb)\n",
    "                    \n",
    "    print(f\"Found {n_person} persons\")\n",
    "    print(f\"Found {n_neg_frames} negative frames\")\n",
    "    print(f\"Found {n_objects} total objects\")\n",
    "    print(f\"Parts Distribution: \", parts_count)\n",
    "    \n",
    "    return anno_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_motion_blur(image, k_size = 30):\n",
    "    # Returns the augmented image\n",
    "    # Specify the kernel size. \n",
    "    # The greater the size, the more the motion. \n",
    "    kernel_size = k_size\n",
    "    \n",
    "    # Vertical Kernel \n",
    "    kernel_v = np.zeros((kernel_size, kernel_size))\n",
    "    # Horizontal Kernel\n",
    "    kernel_h = np.copy(kernel_v) \n",
    "    # Diagonal Kernel\n",
    "    kernel_diag = np.copy(kernel_v)\n",
    "    # Opposite Diagonal Kernel\n",
    "    kernel_opposite_diag = np.copy(kernel_v)\n",
    "\n",
    "    # Fill the middle row with ones. \n",
    "    kernel_v[:, int((kernel_size - 1)/2)] = np.ones(kernel_size)\n",
    "    kernel_h[int((kernel_size - 1)/2), :] = np.ones(kernel_size)\n",
    "    kernel_diag = cv2.line(kernel_diag, (0, 0), (kernel_size-1, kernel_size-1), (1), 1)\n",
    "    kernel_opposite_diag = cv2.line(kernel_opposite_diag, (kernel_size-1, 0), (0, kernel_size-1) , (1), 1)\n",
    "\n",
    "    # Normalize. \n",
    "    kernel_v /= kernel_size \n",
    "    kernel_h /= kernel_size \n",
    "    kernel_diag /= kernel_size\n",
    "    kernel_opposite_diag /= kernel_size\n",
    "\n",
    "    # Apply the vertical kernel. \n",
    "    vertical_mb = cv2.filter2D(image, -1, kernel_v) \n",
    "    \n",
    "    # Apply the horizontal kernel. \n",
    "    horizonal_mb = cv2.filter2D(image, -1, kernel_h) \n",
    "    \n",
    "    # Apply the diagonal kernel\n",
    "    diagonal_mb = cv2.filter2D(image, -1, kernel_diag)\n",
    "    \n",
    "    opposite_diagonal_mb = cv2.filter2D(image, -1, kernel_opposite_diag)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return vertical_mb, horizonal_mb, diagonal_mb, opposite_diagonal_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_annots(anno_dict, containing_folder=\"darknet_annots/pfhul/\", base_path = \"darknet/\"):\n",
    "    darknet_annot_path = base_path + containing_folder\n",
    "\n",
    "    # Write the annot dict to file:\n",
    "    for key in anno_dict.keys():\n",
    "        out_path = darknet_annot_path + key + \".txt\"\n",
    "        with open(out_path, 'w+') as out:\n",
    "            for line in anno_dict[key]:\n",
    "                # Write all lines to \n",
    "                line_s = \"\"\n",
    "                for s in line:\n",
    "                    line_s += str(s) + \" \"\n",
    "                line_s += \"\\n\"\n",
    "                out.write(line_s)\n",
    "    print(\"Annotations were written to file successfully !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_test_train_files(anno_dict, containing_folder_name, base_path=\"darknet/\", split=0.7):\n",
    "    train_path = base_path + f\"train_test_split/{containing_folder_name}_train.txt\"\n",
    "    test_path = base_path + f\"train_test_split/{containing_folder_name}_test.txt\"\n",
    "    \n",
    "    anno_keys = list(anno_dict.keys())\n",
    "    train_count = int(len(anno_dict.keys()) * split)\n",
    "    with open(train_path, \"w+\") as train:\n",
    "        for key in anno_keys[:train_count]:\n",
    "            train.write(f\"data/{containing_folder_name}/{key}.jpg\\n\")\n",
    "    with open(test_path, \"w+\") as test:\n",
    "        for key in anno_keys[train_count:]:\n",
    "            test.write(f\"data/{containing_folder_name}/{key}.jpg\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copy_images(anno_dict, obj_name):\n",
    "    for im_id in anno_dict.keys():\n",
    "        # If the image is augmented then dont copy it to the folder because it already exists\n",
    "        if im_id.split(\"_\")[0] == \"aug\":\n",
    "            continue\n",
    "        im_path = \"../VOC2010/VOCdevkit/VOC2010/JPEGImages/\" + im_id + \".jpg\"\n",
    "        copy(im_path, f\"./darknet/darknet_annots/{obj_name}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_everything(obj_name):\n",
    "    anno_paths = glob.glob(\"Annotations_Part/*.mat\")\n",
    "    \n",
    "    upper_body = ('torso', 'luarm', 'llarm', 'ruarm', 'rlarm', 'lhand', 'rhand')\n",
    "    lower_body = ('luleg', 'llleg', 'ruleg', 'rlleg', 'rfoot', 'lfoot')\n",
    "    body = ('torso', 'lhand', 'rhand', 'luleg', 'llleg', 'ruleg', 'rlleg', 'rfoot', 'lfoot')\n",
    "\n",
    "    face = ('frontal_face')\n",
    "    pfhul = [('all'), face, ('head'), upper_body, lower_body]\n",
    "    pf = [('all'), face]\n",
    "    person = [('all')]\n",
    "    \n",
    "    bf = [('torso'), face]\n",
    "    \n",
    "    anno_dict = parts2darknet(bf, anno_paths, darknet_path=\"darknet/darknet_annots/{0}/\".format(obj_name))    \n",
    "    write_annots(anno_dict, containing_folder=f\"darknet_annots/{obj_name}/\")\n",
    "    generate_test_train_files(anno_dict, obj_name)\n",
    "    copy_images(anno_dict, obj_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10103/10103 [13:56<00:00, 12.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7803 persons\n",
      "Found 6564 negative frames\n",
      "Found 12958 total objects\n",
      "Parts Distribution:  [7368, 2859]\n",
      "Annotations were written to file successfully !!\n"
     ]
    }
   ],
   "source": [
    "do_everything(\"bf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
